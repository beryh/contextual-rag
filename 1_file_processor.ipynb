{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Processor\n",
    "1. pdf 파일을 읽어서 chunking 작업을 수행\n",
    "2. 저장된 chunk들에 대해 embedding 작업을 수행\n",
    "3. embedding 된 내용을 vector store (opensearch)에 저장\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1. PDF 처리 후 Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_file = 'data/bedrock-ug.pdf'\n",
    "chunk_size = 1000\n",
    "start_page = 15\n",
    "\n",
    "# Additional Parameters for Contextual Retrieval\n",
    "add_contextual = True\n",
    "document_size = 20000\n",
    "\n",
    "document_name = Path(input_file).resolve().stem\n",
    "document_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Document into chunked format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from helpers.document_helper import DocumentHelper\n",
    "\n",
    "chunked_document = DocumentHelper.split(full_text=DocumentHelper.load_pdf(input_file, start_page=start_page), chunk_size=chunk_size, max_document_length=document_size if add_contextual else -1)\n",
    "\n",
    "# save result into json file\n",
    "output_file = f\"output/{document_name}_{chunk_size}{\"_situated\" if add_contextual else \"\"}_chunks.json\"\n",
    "\n",
    "import json\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunked_document, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Chunks saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Progress Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-0. Load Requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from libs.bedrock_service import BedrockService\n",
    "from libs.opensearch_service import OpensearchService\n",
    "\n",
    "from config import Config\n",
    "config = Config.load()\n",
    "config.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "bedrock_service = BedrockService(config.aws.region, config.aws.profile, config.bedrock.retries, config.bedrock.embed_model_id, config.bedrock.model_id, config.model.max_tokens, config.model.temperature, config.model.top_p)\n",
    "opensearch_service = OpensearchService(config.aws.region, config.aws.profile, config.opensearch.prefix, config.opensearch.domain_name, config.opensearch.document_name, config.opensearch.user, config.opensearch.password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Situate Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "temperature = 0.0\n",
    "top_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "if add_contextual:\n",
    "    chunked_file = f\"output/{document_name}_{chunk_size}{\"_situated\" if add_contextual else \"\"}_chunks.json\"\n",
    "\n",
    "    with open(chunked_file, 'r', encoding='utf-8') as f:\n",
    "        documents = json.load(f)\n",
    "\n",
    "    total_token_usage = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
    "    documents_token_usage = {}\n",
    "\n",
    "    sys_prompt = \"\"\"\n",
    "    You're an expert at providing a succinct context, targeted for specific text chunks.\n",
    "\n",
    "    <instruction>\n",
    "    - Offer 1-5 short sentences that explain what specific information this chunk provides within the document.\n",
    "    - Focus on the unique content of this chunk, avoiding general statements about the overall document.\n",
    "    - Clarify how this chunk's content relates to other parts of the document and its role in the document.\n",
    "    - If there's essential information in the document that backs up this chunk's key points, mention the details.\n",
    "    </instruction>\n",
    "    \"\"\"\n",
    "    fail_count = 0\n",
    "\n",
    "    for doc_index, document in tqdm(enumerate(documents), leave = False, total=len(documents)):\n",
    "        if fail_count > 10:\n",
    "            break\n",
    "        doc_content = document['content']\n",
    "\n",
    "        if 'token_usage' in document:\n",
    "            doc_token_usage = document['token_usage']\n",
    "        else:\n",
    "            document['token_usage'] = {\"inputTokens\": 0, \"outputTokens\": 0, \"totalTokens\": 0}\n",
    "        \n",
    "        for chunk in tqdm(document['chunks']):\n",
    "            if 'simulated' in chunk:\n",
    "                continue\n",
    "            document_context_prompt = f\"\"\"\n",
    "            <document>\n",
    "            {doc_content}\n",
    "            </document>\n",
    "            \"\"\"\n",
    "\n",
    "            chunk_content = chunk['content']\n",
    "            chunk_context_prompt = f\"\"\"\n",
    "            Here is the chunk we want to situate within the whole document:\n",
    "\n",
    "            <chunk>\n",
    "            {chunk_content}\n",
    "            </chunk>\n",
    "\n",
    "            Skip the preamble and only provide the consise context.\n",
    "            \"\"\"\n",
    "            usr_prompt = [{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": [\n",
    "                        {\"text\": document_context_prompt},\n",
    "                        {\"text\": chunk_context_prompt}\n",
    "                    ]\n",
    "                }]\n",
    "            \n",
    "            try:\n",
    "                response = bedrock_service.converse(\n",
    "                    messages=usr_prompt, \n",
    "                    system_prompt=sys_prompt,\n",
    "                    temperature=temperature,\n",
    "                    top_p=top_p,\n",
    "                    max_tokens=4096\n",
    "                )\n",
    "                situated_context = response['output']['message']['content'][0]['text'].strip()\n",
    "                chunk['content'] = f\"Context:\\n{situated_context}\\n\\nChunk:\\n{chunk['content']}\"\n",
    "                chunk['simulated'] = True\n",
    "\n",
    "                if 'usage' in response:\n",
    "                    usage = response['usage']\n",
    "                    for key in ['inputTokens', 'outputTokens', 'totalTokens']:\n",
    "                        document['token_usage'][key] += usage.get(key, 0)\n",
    "                print(f\"completed generating context for chunk [{doc_index}_{chunk['chunk_id']}]\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating context for chunk [{doc_index}_{chunk['chunk_id']}]: {e}\")\n",
    "                fail_count += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "    with open(chunked_file, \"w\", encoding='utf-8') as f:\n",
    "        json.dump(documents, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configure Index\n",
    "index_prefix = \"aws_\"\n",
    "index_name = (f\"{index_prefix}contextual_{document_name}\" if add_contextual and not document_name.startswith(\"contextual_\") else document_name) + f\"_{chunk_size}\"\n",
    "\n",
    "overwrite_index = True\n",
    "\n",
    "opensearch_index_configuration = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.algo_param.ef_search\": 512\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"metadata\": {\n",
    "                \"properties\": {\n",
    "                    \"source\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    },\n",
    "                    \"doc_id\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    },\n",
    "                    \"timestamp\": {\n",
    "                        \"type\": \"date\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"standard\"\n",
    "            },\n",
    "            \"content_embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,\n",
    "                \"method\": {\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 512,\n",
    "                        \"m\": 16\n",
    "                    },\n",
    "                    \"space_type\": \"l2\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if overwrite_index:\n",
    "    if opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
    "        opensearch_service.opensearch_client.indices.delete(index=index_name)\n",
    "    \n",
    "    opensearch_service.opensearch_client.indices.create(index=index_name, body=opensearch_index_configuration)\n",
    "else:\n",
    "    if not opensearch_service.opensearch_client.indices.exists(index=index_name):\n",
    "        opensearch_service.opensearch_client.indices.create(index=index_name, body=opensearch_index_configuration)\n",
    "\n",
    "index_pattern = f\"{index_prefix}*\" if index_prefix else \"*\"\n",
    "indices = opensearch_service.opensearch_client.cat.indices(index=index_pattern, format=\"json\")\n",
    "\n",
    "indices_name = [item['index'] for item in indices]\n",
    "indices_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Embed Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from libs.bedrock_service import BedrockService\n",
    "from datetime import datetime\n",
    "\n",
    "bedrock_service = BedrockService(config.aws.region, config.aws.profile, config.bedrock.retries, config.bedrock.embed_model_id, config.bedrock.model_id, config.model.max_tokens, config.model.temperature, config.model.top_p)\n",
    "\n",
    "with open(chunked_file, 'r', encoding='utf-8') as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "embedded_documents = []\n",
    "\n",
    "for document in tqdm(documents):\n",
    "    doc_id = document['doc_id']\n",
    "    embedded_chunks = []\n",
    "\n",
    "    for chunk in tqdm(document['chunks']):\n",
    "        context = chunk['content']\n",
    "        chunk_embedding = bedrock_service.embedding(text=context)\n",
    "        if chunk_embedding:\n",
    "            chunk_id = chunk['chunk_id']\n",
    "            _id = f\"{doc_id}_{chunk_id}\"\n",
    "            embedded_chunk = {\n",
    "                \"metadata\": {\n",
    "                    \"source\": document_name, \n",
    "                    \"doc_id\": doc_id,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                },\n",
    "                \"content\": chunk['content'],\n",
    "                \"content_embedding\": chunk_embedding\n",
    "            }\n",
    "            embedded_chunks.append(embedded_chunk)\n",
    "\n",
    "            opensearch_service.opensearch_client.index(\n",
    "                index=index_name,\n",
    "                body=embedded_chunk\n",
    "            )\n",
    "            \n",
    "        embedded_documents.append({\n",
    "            \"_id\": _id,\n",
    "            \"embedded_chunks\": embedded_chunks\n",
    "        })\n",
    "        \n",
    "print(f\"Successfully embedded and stored documents in index '{index_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. Test Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "question = \"What is Bedrock?\"\n",
    "\n",
    "question_embedding = bedrock_service.embedding(text=question)\n",
    "knn = opensearch_service.search_by_knn(question_embedding, 'contextual_bedrock-ug_1000')\n",
    "knn"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

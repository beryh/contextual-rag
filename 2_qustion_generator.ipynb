{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (8.29.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.venv/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.12/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup File Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bedrock-ug'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "input_file = \"data/bedrock-ug.pdf\"\n",
    "chunk_size = 1000\n",
    "start_page = 0\n",
    "end_page = -1\n",
    "\n",
    "document_name = Path(input_file).resolve().stem\n",
    "document_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-2\n"
     ]
    }
   ],
   "source": [
    "from libs.bedrock_service import BedrockService\n",
    "from config import Config\n",
    "config = Config.load()\n",
    "\n",
    "bedrock_service = BedrockService(config.aws.region, config.aws.profile, config.bedrock.retries, config.bedrock.embed_model_id, config.bedrock.model_id, config.model.max_tokens, config.model.temperature, config.model.top_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split Document into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1671/1671 [01:19<00:00, 20.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Load: data/bedrock-ug.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 93.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from helpers.document_helper import DocumentHelper\n",
    "chunked_document = DocumentHelper.split(DocumentHelper.load_pdf(input_file, start_page, end_page), chunk_size, -1)\n",
    "chunks = chunked_document[0]['chunks']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build Prompt and Tool Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_template = {\n",
    "    \"complex\": \"\"\"\n",
    "        You are an expert at generating practical questions based on given documentation.\n",
    "        Your task is to generate complex, reasoning questions and answers.\n",
    "\n",
    "        Follow these rules:\n",
    "        1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
    "        2. Ensure questions are relevant, concise, preferably under 25 words, and fully answerable with the provided information\n",
    "        3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
    "        4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
    "        5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
    "    \"\"\",\n",
    "    \"simple\": \"\"\"\n",
    "        You are an expert at generating practical questions based on given documentation.\n",
    "        Your task is to create simple, directly answerable questions from the given context.\n",
    "\n",
    "        Follow these rules:\n",
    "        1. Generate questions that reflect real user information needs related to the document's subject matter (e.g., technical docs : feature availability, implementation details)\n",
    "        2. Ensure questions are relevant, concise, preferably under 10 words, and fully answerable with the provided information\n",
    "        3. Focus on extracting key information that users are likely to seek, while avoiding narrow or less important questions.\n",
    "        4. When provided with code blocks, focus on understanding the overall functionality rather than the specific syntax or variables. Feel free to request examples of how to use key APIs or features.\n",
    "        5. Do not use phrases like 'based on the provided context' or 'according to the context'.\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_config = {\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"toolSpec\": {\n",
    "                \"name\": \"QuestionAnswerGenerator\",\n",
    "                \"description\": \"Generates questions and answers based on the given context.\",\n",
    "                \"inputSchema\": {\n",
    "                    \"json\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"question\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The generated question\"\n",
    "                            },\n",
    "                            \"answer\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The answer to the generated question\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"question\", \"answer\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generate Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs = 5\n",
    "\n",
    "output_file = f\"output/{document_name}_sample_questions.jsonl\"\n",
    "output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f91ef2033a340a487519a41637f362d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'question_id': '154444d4-a694-4dea-bb47-fac2b39ba251',\n",
       "  'question': \"How do the S3 storage and S3 retrieval nodes in Amazon Bedrock's prompt flow differ in their functionality, inputs, and outputs, and what potential use case might combine these two node types?\",\n",
       "  'ground_truth': \"The S3 storage and S3 retrieval nodes in Amazon Bedrock's prompt flow have distinct but complementary functions:\\n\\n1. S3 storage node:\\n   - Function: Stores data in an Amazon S3 location\\n   - Inputs: Content to store and the object key\\n   - Output: URI of the S3 location\\n   - Configuration: Specifies the S3 bucket for data storage\\n\\n2. S3 retrieval node:\\n   - Function: Retrieves data from an Amazon S3 location\\n   - Input: Object key\\n   - Output: Content from the S3 location (currently limited to UTF-8 encoded strings)\\n   - Configuration: Specifies the S3 bucket for data retrieval\\n\\nA potential use case combining these nodes could be a multi-step data processing workflow:\\n1. Use the S3 storage node to save intermediate results or large datasets generated during the flow.\\n2. Later in the flow or in a separate flow, use the S3 retrieval node to fetch the stored data for further processing or analysis.\\n\\nThis combination allows for efficient handling of large datasets, enables data persistence between flow executions, and facilitates modular design of complex data processing pipelines in Amazon Bedrock's prompt flow system.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': 'el ID to use if you want to generate a response based on the retrieved results. To return the retrieved results as an array, omit the model ID. The input into the node is the query to the knowledge base. The output is either the model response, as a string, or an array of the retrieved results. Node types in prompt flow 912 Amazon Bedrock User Guide The following shows the general structure of a knowledge base FlowNode object: { \"name\": \"string\", \"type\": \"KnowledgeBase\", \"inputs\": [ { \"name\": \"retrievalQuery\", \"type\": \"String\", \"expression\": \"string\" } ], \"outputs\": [ { \"name\": \"retrievalResults\", \"type\": \"Array | String\" } ], \"configuration\": { \"knowledgeBase\": { \"knowledgeBaseId\": \"string\", \"modelId\": \"string\" } } } S3 storage node An S3 storage node lets you store data in the flow to an Amazon S3 location. In the configuration, you specify the S3 bucket to use for data storage. The inputs into the node are the content to store and the object key. The node returns the URI of the S3 location as its output. and the object key. The node returns the URI of the S3 location as its output. The following shows the general structure of an S3 storage FlowNode object: { \"name\": \"string\", \"type\": \"Storage\", \"inputs\": [ { \"name\": \"content\", \"type\": \"String | Number | Boolean | Object | Array\", \"expression\": \"string\" }, Node types in prompt flow 913 Amazon Bedrock User Guide { \"name\": \"objectKey\", \"type\": \"String\", \"expression\": \"string\" } ], \"outputs\": [ { \"name\": \"s3Uri\", \"type\": \"String\" } ], \"configuration\": { \"retrieval\": { \"serviceConfiguration\": { \"s3\": { \"bucketName\": \"string\" } } } } } S3 retrieval node An S3 retrieval node lets you retrieve data from an Amazon S3 location to introduce to the flow. In the configuration, you specify the S3 bucket from which to retrieve data. The input into the node is the object key. The node returns the content in the S3 location as the output. Note Currently, the data in the S3 location must be a UTF-8 encoded string. The following shows the general structure of an S3 retrieval FlowNode object: { \"name\": \"string\", \"type\": \"Retrieval\", \"inputs\": [ { \"name\": \"objectKey\", Node types in prompt flow 914 Amazon Bedrock User Guide \"type\": \"String\", \"expression\": \"string\" } ], \"outputs\": [ { \"name\": \"s3Content\", \"type\": \"String\" } ], \"configuration\": { \"retrieval\": { \"serviceConfiguration\": { \"s3\": { \"bucketName\": \"string\" } } } } } Lambda function node A Lambda function node lets you call a Lambda function in which you can define code to carry out business logic. all a Lambda function in which you can define code to carry out business logic. When you include a Lambda node in a prompt flow, Amazon Bedrock sends an input event to the Lambda function that you specify. In the configuration, specify the Amazon Resource Name (ARN) of the Lambda function. Define inputs to send in the Lambda input event. You can write code based on these inputs and define what the function returns. The function response is returned in the output. The following shows the general structure of a Λ function FlowNode object: { \"name\": \"string\", \"type\": \"LambdaFunction\", \"inputs\": [ { \"name\": \"string\", \"type\": \"String | Number | Boolean | Object | Array\", \"expression\": \"string\" }, ... Node types in prompt flow 915 Amazon Bedrock User Guide ], \"outputs\": [ { \"name\": \"functionResponse\", \"type\": \"String | Number | Boolean | Object | Array\" } ], \"configuration\": { \"lambdaFunction\": { \"lambdaArn\": \"string\" } } } Lambda input event for a prompt flow The input event sent to a Lambda function in a Lambda node is of the following format: { \"messageVersion\": \"1.0\", \"flow\": { \"flowArn\": \"string\", \"flowAliasArn\": \"string\" }, \"node\": { \"name\": \"string\", \"nodeInputs\": [ { \"name\": \"string\", \"type\": \"String | Number | Boolean | Object | Array\", \"expression\": \"string\", \"value\": ...'},\n",
       " {'question_id': '10ce6181-3123-4af7-9ed0-b493b027cc13',\n",
       "  'question': 'What does the woods symbolize in Robert Frost\\'s poem \"Stopping by Woods on a Snowy Evening\"?',\n",
       "  'ground_truth': 'The woods symbolize death and the afterlife, representing both comfort and the unknown.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'ategory name 3}} which can be used to build 56 different predefined structures. Assistant:\"\"\" Enhance creativity, different structures can be connected (Source: Written by AWS) to form new structures, encouraging out-of-the box thinking. Enhance child-parent bonding, parents can play with their children together to foster social skills. </text> Categories are: (1) Toys (2) Beauty and Health (3) Electronics Assistant: Output: <output>Toys</output> (Source of prompt: AWS, model used: Anthropic Claude) Text classification 354 Amazon Bedrock User Guide Question-answer, without context In a question-answer prompt without context, the model must answer the question with its internal knowledge without using any context or document. Prompt Template to Titan User prompt: \"\"\"{{Question}}\"\"\" What is Robert Frost\\'s \"Stopping by the woods on a snowy evening\" about metaphorically? Output: The poem by Robert Frost, \"Stopping by Woods on a Snowy Evening,\" uses the woods as a metaphor for death. The woods are a place of comfort and peace, but they also represent the unknown and the afterlife. ce of comfort and peace, but they also represent the unknown and the afterlife. The poem suggests that death is not something to be feared, but rather a natural part of life that we should all embrace. The poem begins with the speaker stopping by the woods on a snowy evening, a moment of contemplation and reflection. The speaker notices the beauty of the snow-covered woods and is reminded of the fleeting nature of life. The poem continues as the speaker considers the temptation of continuin g on to his destination, but instead chooses to pause and take in the beauty of the woods. The woods are used as a metaphor for the unknown and the afterlife. Question-answer, without context 355 Amazon Bedrock User Guide The speaker wonders what lies beyond the trees, and whether there is anything after death. The poem suggests that death is not something to be feared, but rather a natural part of life that we should all embrace. The final lines of the poem are perhaps the most famous and poignant: \"And miles to go before I sleep, and miles to go before I sleep.\" These lines suggest that even though the speaker has paused to appreciate the beauty of the woods, they still have a long journey ahead of them. preciate the beauty of the woods, they still have a long journey ahead of them. The poem is a reminder that life is fleeting, and that we should take the time to appreciate the beauty around us, even as we work towards our goals and ambitions. Overall, \"Stopping by Woods on a Snowy Evening\" is a beautiful and thought-provoking poem that uses the metaphor of the woods to explore the concept of death and the afterlife. (Source of prompt: AWS, model used: Amazon Titan Text) Model encouragement can also help in question-answer tasks. Prompt Template to Titan User prompt: \"\"\"{{Model encouragement}} You are excellent at answering {{Question}}\"\"\" questions, and it makes you happy when you provide the correct answer. Question-answer, without context 356 Amazon Bedrock User Guide What moon in the solar system is most likely to host life? Output: The moon of Titan, which is a moon of the planet Saturn, is the most likely to support life in the solar system. Titan is the only known moon in the solar system to have a dense atmosphere and a liquid hydrosphere, and scientists believe that it may contain the chemical building blocks for life.'},\n",
       " {'question_id': '281ff923-00c1-43d0-bf17-35667fe6bf91',\n",
       "  'question': 'How does Amazon Bedrock handle vector embeddings and storage configurations when creating a knowledge base, and what options are available for different database types?',\n",
       "  'ground_truth': \"When creating a knowledge base in Amazon Bedrock, vector embeddings and storage configurations are handled through several options:\\n\\n1. Vector embeddings: Users must specify an embedding model in the 'embeddingModelArn' field of the 'knowledgeBaseConfiguration' object. This model converts data into vector embeddings.\\n\\n2. Storage configurations: Amazon Bedrock offers multiple options for vector storage:\\n   a. Amazon OpenSearch Service (managed by Amazon Bedrock)\\n   b. User's own vector store, configured in the 'storageConfiguration' object\\n\\n3. Database options for user-managed vector stores:\\n   - Amazon OpenSearch Service: Use 'opensearchServerlessConfiguration'\\n   - Pinecone: Use 'pineconeConfiguration'\\n   - Redis Enterprise Cloud: Use 'redisEnterpriseCloudConfiguration'\\n   - Amazon Aurora: Use 'rdsConfiguration'\\n   - MongoDB Atlas: Use 'mongodbConfiguration'\\n\\n4. Data sources: After creating the knowledge base, users must create a data source using the 'CreateDataSource' request, specifying connection information, chunking configuration, and data deletion policy.\\n\\nThis flexible approach allows users to choose the most suitable vector embedding model and storage solution for their specific use case while integrating with various database types.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': \"ils of your knowledge base. Choose Edit in any section that you need to modify. When you are satisfied, select Create knowledge base. Create a knowledge base 583 Amazon Bedrock User Guide 10. The time it takes to create the knowledge base depends on your specific configurations. When the knowledge base creation has completed, the status of the knowledge base changes to either state it is ready or available. API To create a knowledge base, send a CreateKnowledgeBase request with a Agents for Amazon Bedrock build-time endpoint and provide the name, description, instructions for what it should do, and the foundation model for it to orchestrate with. Note If you prefer to let Amazon Bedrock create and manage a vector store for you in Amazon OpenSearch Service, use the console. For more information, see Create an Amazon Bedrock knowledge base. • Provide the ARN with permissions to create a knowledge base in the roleArn field. • Provide the vector embeddings model to use in the embeddingModelArn field in the knowledgeBaseConfiguration object. to use in the embeddingModelArn field in the knowledgeBaseConfiguration object. See supported models for knowledge bases. You must enable model access to use a model that's supported for knowledge bases. Take note of your model Amazon Resource Name (ARN) that's required for converting your data into vector embeddings. Copy the model ID for your chosen model for knowledge bases and construct the model ARN using the model (resource) ID, following the provided ARN examples for your model resource type. • Provide the configuration for your vector store in the storageConfiguration object. For more information, see Prerequisites for your own vector store for a knowledge base • For an Amazon OpenSearch Service database, use the opensearchServerlessConfiguration object. • For a Pinecone database, use the pineconeConfiguration object. • For a Redis Enterprise Cloud database, use the redisEnterpriseCloudConfiguration object. • For an Amazon Aurora database, use the rdsConfiguration object. • For an MongoDB Atlas database, use the mongodbConfiguration object. n object. • For an MongoDB Atlas database, use the mongodbConfiguration object. Create a knowledge base 584 Amazon Bedrock User Guide After you create a knowledge base, create a data source containing the documents or content for your knowledge base. To create the data source send a CreateDataSource request. See Supported data sources to select your data source and follow the API connection configuration example. • Provide the connection information for the data source files in the dataSourceConfiguration field. • Specify how to chunk the data sources in the vectorIngestionConfiguration field. Note You can't change the chunking configuration after you create the data source. • Provide the dataDeletionPolicy for your data source. You can DELETE all data from your data source that’s converted into vector embeddings upon deletion of a knowledge base or data source resource. This flag is ignored if an AWS account is deleted. You can RETAIN all data from your data source that’s converted into vector embeddings upon deletion of a knowledge base or data source resource.\"},\n",
       " {'question_id': 'a7544e00-a5e2-441d-97e3-0c55b9b71ae5',\n",
       "  'question': 'How can you increase model inference rates when creating an agent alias?',\n",
       "  'ground_truth': 'You can increase model inference rates by selecting Provisioned Throughput (PT) and choosing a previously purchased provisioned model when creating the agent alias.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"hoose Create. 4. Enter a unique Alias name and provide an optional Description. 5. Under Associate a version, choose one of the following options: • To create a new version, choose Create a new version and to associate it to this alias. • To use an existing version, choose Use an existing version to associate this alias. From the dropdown menu, choose the version that you want to associate the alias to. 6. Under Select throughput, select one of the following options: • To let your agent run model inference at the rates set for your account, select On- demand (ODT). For more information, see Quotas for Amazon Bedrock. • To let your agent run model inference at an increased rate using a Provisioned Throughput that you previously purchased for the model, select Provisioned Throughput (PT) and then select a provisioned model. For more information, see Increase model invocation capacity with Provisioned Throughput in Amazon Bedrock. 7. Select Create alias. Deploy and integrate agent into your application 888 Amazon Bedrock User Guide API To create an alias for an agent, send a CreateAgentAlias request (see link for request and response formats and field details) with an Agents for Amazon Bedrock build-time endpoint. rmats and field details) with an Agents for Amazon Bedrock build-time endpoint. The following fields are required: Field Use case agentId To specify the ID of the agent for which to create an alias. agentName To specify a name for the alias. The following fields are optional: Field Use case description To provide a description of the alias. routingConfiguration To specify a version to associate the alias with (leave blank to create a new version) and a Provisioned Throughput to associate with the alias. clientToken Identifier to ensure the API request completes only once. tags To associate tags with the alias. See code examples 2. Deploy your agent by setting up your application to make an InvokeAgent request (see link for request and response formats and field details) with an Agents for Amazon Bedrock runtime Deploy and integrate agent into your application 889 Amazon Bedrock User Guide endpoint. In the agentAliasId field, specify the ID of the alias pointing to the version of the agent that you want to use. the ID of the alias pointing to the version of the agent that you want to use. View information about versions of agents in Amazon Bedrock After you create a version of your agent, you can view information about it or delete it. You can only create a new version of an agent by creating a new alias. To learn how to view information about the versions of an agent, select the tab corresponding to your method of choice and follow the steps: Console To view information about a version of an agent 1. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/ bedrock/. 2. Select Agents from the left navigation pane. Then, choose an agent in the Agents section. 3. Choose the version to view from the Versions section. 4. To view details about the model, action groups, or knowledge bases attached to version of the agent, choose the name of the information that you want to view. You can't modify any part of a version.\"},\n",
       " {'question_id': '77ca8ce9-f812-4d67-81b2-0feb93047c9a',\n",
       "  'question': 'How does the inpainting process in Amazon Titan models differ from text-to-image generation, and what unique parameters does it require to achieve targeted image modifications?',\n",
       "  'ground_truth': \"Inpainting in Amazon Titan models differs from text-to-image generation by allowing targeted modifications to specific parts of an existing image, rather than creating an entirely new image from scratch. It requires unique parameters such as:\\n\\n1. An input image (base64-encoded string)\\n2. A mask, which can be defined in two ways:\\n   a. maskPrompt: A text description of the area to be masked\\n   b. maskImage: A base64-encoded string marking pixels as inside (0,0,0) or outside (255,255,255) the mask\\n3. An optional 'text' parameter to define what to change inside the mask\\n4. An optional 'returnMask' boolean parameter\\n\\nUnlike text-to-image generation, inpainting maintains the original image's size and only modifies the masked area. Both methods share some parameters like 'negativeText' for exclusions and 'imageGenerationConfig' for controlling aspects like the number of images and dimensions. Inpainting offers more precise control over image modifications, making it suitable for targeted edits or additions to existing images.\",\n",
       "  'question_type': 'complex',\n",
       "  'context': 'nges from 0 to 255 (for example, (255 255 0) would represent the color yellow). These channels are encoded in base64. The image you use must be in JPEG or PNG format. If you carry out inpainting or outpainting, you also define a mask, a region or regions that define parts of the image to be modified. You can define the mask in one of two ways. • maskPrompt – Write a text prompt to describe the part of the image to be masked. • maskImage – Input a base64-encoded string that defines the masked regions by marking each pixel in the input image as (0 0 0) or (255 255 255). • A pixel defined as (0 0 0) is a pixel inside the mask. • A pixel defined as (255 255 255) is a pixel outside the mask. You can use a photo editing tool to draw masks. You can then convert the output JPEG or PNG image to base64-encoding to input into this field. Otherwise, use the maskPrompt field instead to allow the model to infer the mask. Select a tab to view API request bodies for different image generation use-cases and explanations of the fields. bodies for different image generation use-cases and explanations of the fields. Amazon Titan models 84 Amazon Bedrock User Guide Text-to-image generation (Request) A text prompt to generate the image must be <= 512 characters. Resolutions <= 1,408 on the longer side. negativeText (Optional) – A text prompt to define what not to include in the image that is <= 512 characters. See the table below for a full list of resolutions. { \"taskType\": \"TEXT_IMAGE\", \"textToImageParams\": { \"text\": \"string\", \"negativeText\": \"string\" }, \"imageGenerationConfig\": { \"numberOfImages\": int, \"height\": int, \"width\": int, \"cfgScale\": float, \"seed\": int } } The textToImageParams fields are described below. • text (Required) – A text prompt to generate the image. • negativeText (Optional) – A text prompt to define what not to include in the image. Note Don\\'t use negative words in the negativeText prompt. For example, if you don\\'t want to include mirrors in an image, enter mirrors in the negativeText prompt. Don\\'t enter no mirrors. in an image, enter mirrors in the negativeText prompt. Don\\'t enter no mirrors. Inpainting (Request) text (Optional) – A text prompt to define what to change inside the mask. If you don\\'t include this field, the model tries to replace the entire mask area with the background. Must be <= 512 characters. negativeText (Optional) – A text prompt to define what not to include in the image. Must be <= 512 characters. The size limits for the input image and input mask are <= 1,408 on the longer side of image. The output size is the same as the input size. Amazon Titan models 85 Amazon Bedrock User Guide { \"taskType\": \"INPAINTING\", \"inPaintingParams\": { \"image\": \"base64-encoded string\", \"text\": \"string\", \"negativeText\": \"string\", \"maskPrompt\": \"string\", \"maskImage\": \"base64-encoded string\", \"returnMask\": boolean # False by default }, \"imageGenerationConfig\": { \"numberOfImages\": int, \"height\": int, \"width\": int, \"cfgScale\": float } } The inPaintingParams fields are described below. The mask defines the part of the image that you want to modify.'},\n",
       " {'question_id': 'c2fcc861-ccae-4595-b590-6944f929d696',\n",
       "  'question': 'What happens if a guardrail blocks the input prompt?',\n",
       "  'ground_truth': 'If a guardrail blocks the input prompt, you will be charged for the guardrail evaluation, but there will be no charges for foundation model inference calls.',\n",
       "  'question_type': 'simple',\n",
       "  'context': \"ontent filters, denied topics, sensitive information filters, and word filters. • A guardrail can be configured with a single policy, or a combination of multiple policies. • A guardrail can be used with any text-only foundation model (FM) by referencing the guardrail during the model inference. • You can use guardrails with Amazon Bedrock Agents and Amazon Bedrock Knowledge Bases. When using a guardrail, it work as follows during the inference call: • The input is evaluated against the configured policies specified in the guardrail. Furthermore, for improved latency, the input is evaluated in parallel for each configured policy. • If the input evaluation results in a guardrail intervention, a configured blocked message response is returned and the foundation model inference is discarded. • If the input evaluation succeeds, the model response is then subsequently evaluated against the configured policies in the guardrail. • If the response results in a guardrail intervention or violation, it will be overridden with pre- configured blocked messaging or masking of the sensitive information. with pre- configured blocked messaging or masking of the sensitive information. • If the response's evaluation succeeds, the response is returned to the application without any modifications. For information on Amazon Bedrock Guardrails pricing, see the Amazon Bedrock pricing. How charges are calculated for Amazon Bedrock Guardrails Charges for Amazon Bedrock Guardrails will be incurred only for the policies configured in the guardrail. The price for each policy type is available at Amazon Bedrock Pricing. If guardrails blocks the input prompt, you will be charged for the guardrail evaluation. There will be no charges for 401 Amazon Bedrock User Guide foundation model inference calls. If guardrails blocks the model response, you will be charged for guardrails evaluation of the input prompt and the model response. In this case, you will be charged for the foundation model inference calls as well the model response that was generated prior to guardrails evaluation. Supported regions and models for Amazon Bedrock Guardrails Amazon Bedrock Guardrails is supported in the following regions: Region US East (N. on Bedrock Guardrails is supported in the following regions: Region US East (N. Virginia) US East (Ohio) US West (Oregon) AWS GovCloud (US-West) Canada (Central) South America (São Paulo) Europe (Frankfurt) Europe (Ireland) (gated access) Europe (London) Europe (Paris) Asia Pacific (Singapore) (gated access) Asia Pacific (Tokyo) Asia Pacific (Seoul) Asia Pacific (Sydney) Asia Pacific (Mumbai) Supported regions and models for Amazon Bedrock Guardrails 402 Amazon Bedrock User Guide You can use Amazon Bedrock Guardrails with the following models: Model name Model ID Jamba-Instruct v1 ai21.jamba-instruct-v1:0 Jamba 1.5 Large v1 ai21.jamba-1-5-large-v1:0 Jamba 1.5 Mini v1 ai21.jamba-1-5-mini-v1:0 Anthropic Claude Instant v1 anthropic.claude-instant-v1 Anthropic Claude v1.0 anthropic.claude-v1 Anthropic Claude v2.0 anthropic.claude-v2 Anthropic Claude v2.1 anthropic.claude-v2:1 Anthropic Claude 3 Haiku anthropic.claude-3-haiku-20240307-v1 Anthropic Claude 3 Opus anthropic.claude-3-opus-20240229-v1\"},\n",
       " {'question_id': 'a36c7511-274d-4558-805b-0fe8789213cb',\n",
       "  'question': 'How does the Python SDK example for invoking Cohere Command on Amazon Bedrock handle potential errors, and what additional step could improve error handling in the Java SDK example?',\n",
       "  'ground_truth': 'The Python SDK example handles potential errors by using a try-except block that catches both ClientError and general Exceptions. It prints an error message with the specific reason and exits the program if an error occurs. In contrast, the Java SDK example only catches SdkClientException. To improve error handling in the Java SDK example, it could be enhanced to catch a broader range of exceptions, such as JsonException for JSON parsing errors, and provide more detailed error messages. Additionally, implementing a similar exit strategy as in the Python example would prevent the program from continuing execution after encountering an error.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'cohere.command-r-v1:0\"; // The InvokeModel API uses the model\\'s native payload. // Learn more about the available inference parameters and response fields at: // https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters- cohere-command-r-plus.html var nativeRequestTemplate = \"{ \\\\\"message\\\\\": \\\\\"{{prompt}}\\\\\" }\"; Cohere Command 1466 Amazon Bedrock User Guide // Define the prompt for the model. var prompt = \"Describe the purpose of a \\'hello world\\' program in one line.\"; // Embed the prompt in the model\\'s native request payload. String nativeRequest = nativeRequestTemplate.replace(\"{{prompt}}\", prompt); try { // Encode and send the request to the Bedrock Runtime. var response = client.invokeModel(request -> request .body(SdkBytes.fromUtf8String(nativeRequest)) .modelId(modelId) ); // Decode the response body. var responseBody = new JSONObject(response.body().asUtf8String()); // Retrieve the generated text from the model\\'s response. var text = new JSONPointer(\"/ text\").queryFrom(responseBody).toString(); System.out.println(text); return text; } catch (SdkClientException e) { System.err.printf(\"ERROR: Can\\'t invoke \\'%s\\'. ; } catch (SdkClientException e) { System.err.printf(\"ERROR: Can\\'t invoke \\'%s\\'. Reason: %s\", modelId, e.getMessage()); throw new RuntimeException(e); } } public static void main(String[] args) { invokeModel(); } } • For API details, see InvokeModel in AWS SDK for Java 2.x API Reference. Cohere Command 1467 Amazon Bedrock User Guide Python SDK for Python (Boto3) Note There\\'s more on GitHub. Find the complete example and learn how to set up and run in the AWS Code Examples Repository. Use the Invoke Model API to send a text message. # Use the native inference API to send a text message to Cohere Command R and R+. import boto3 import json from botocore.exceptions import ClientError # Create a Bedrock Runtime client in the AWS Region of your choice. client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\") # Set the model ID, e.g., Command R. model_id = \"cohere.command-r-v1:0\" # Define the prompt for the model. prompt = \"Describe the purpose of a \\'hello world\\' program in one line.\" # Format the request payload using the model\\'s native structure. in one line.\" # Format the request payload using the model\\'s native structure. native_request = { \"message\": prompt, \"max_tokens\": 512, \"temperature\": 0.5, } # Convert the native request to JSON. request = json.dumps(native_request) try: # Invoke the model with the request. response = client.invoke_model(modelId=model_id, body=request) except (ClientError, Exception) as e: Cohere Command 1468 Amazon Bedrock User Guide print(f\"ERROR: Can\\'t invoke \\'{model_id}\\'. Reason: {e}\") exit(1) # Decode the response body. model_response = json.loads(response[\"body\"].read()) # Extract and print the response text. response_text = model_response[\"text\"] print(response_text) • For API details, see InvokeModel in AWS SDK for Python (Boto3) API Reference. For a complete list of AWS SDK developer guides and code examples, see Using Amazon Bedrock with an AWS SDK. This topic also includes information about getting started and details about previous SDK versions. Invoke Cohere Command on Amazon Bedrock using the Invoke Model API The following code examples show how to send a text message to Cohere Command, using the Invoke Model API.'},\n",
       " {'question_id': '6b768eaf-36aa-480f-ab63-0cb44d00f37b',\n",
       "  'question': 'What is the recommended token limit for optimal performance with Claude?',\n",
       "  'ground_truth': 'The recommended token limit for optimal performance with Claude is 4,000 tokens.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'mple\": int, \"stop_sequences\": [string] } The following are required parameters. • prompt – (Required) The prompt that you want Claude to complete. For proper response generation you need to format your prompt using alternating \\\\n\\\\nHuman: and \\\\n \\\\nAssistant: conversational turns. For example: \"\\\\n\\\\nHuman: {userQuestion}\\\\n\\\\nAssistant:\" For more information, see Prompt validation in the Anthropic Claude documentation. • max_tokens_to_sample – (Required) The maximum number of tokens to generate before stopping. We recommend a limit of 4,000 tokens for optimal performance. Note that Anthropic Claude models might stop generating tokens before reaching the value of max_tokens_to_sample. Different Anthropic Claude models have different maximum values for this parameter. For more information, see Model comparison in the Anthropic Claude documentation. Default Minimum Maximum 200 0 4096 The following are optional parameters. • stop_sequences – (Optional) Sequences that will cause the model to stop generating. _sequences – (Optional) Sequences that will cause the model to stop generating. Anthropic Claude models stop on \"\\\\n\\\\nHuman:\", and may include additional built-in stop sequences in the future. Use the stop_sequences inference parameter to include additional strings that will signal the model to stop generating text. • temperature – (Optional) The amount of randomness injected into the response. Use a value closer to 0 for analytical / multiple choice, and a value closer to 1 for creative and generative tasks. Anthropic Claude models 134 Amazon Bedrock User Guide Default Minimum Maximum 1 0 1 • top_p – (Optional) Use nucleus sampling. In nucleus sampling, Anthropic Claude computes the cumulative distribution over all the options for each subsequent token in decreasing probability order and cuts it off once it reaches a particular probability specified by top_p. You should alter either temperature or top_p, but not both. Default Minimum Maximum 1 0 1 • top_k – (Optional) Only sample from the top K options for each subsequent token. op_k – (Optional) Only sample from the top K options for each subsequent token. Use top_k to remove long tail low probability responses. Default Minimum Maximum 250 0 500 Response The Anthropic Claude model returns the following fields for a Text Completion inference call. { \"completion\": string, \"stop_reason\": string, \"stop\": string } • completion – The resulting completion up to and excluding the stop sequences. • stop_reason – The reason why the model stopped generating the response. • \"stop_sequence\" – The model reached a stop sequence — either provided by you with the stop_sequences inference parameter, or a stop sequence built into the model. Anthropic Claude models 135 Amazon Bedrock User Guide • \"max_tokens\" – The model exceeded max_tokens_to_sample or the model\\'s maximum number of tokens. • stop – If you specify the stop_sequences inference parameter, stop contains the stop sequence that signalled the model to stop generating text. For example, holes in the following response. { \"completion\": \" Here is a simple explanation of black \", \"stop_reason\": \"stop_sequence\", \"stop\": \"holes\" } If you don\\'t specify stop_sequences, the value for stop is empty.'},\n",
       " {'question_id': '7fad1d7e-c420-42c8-9fad-97d2cc176ec5',\n",
       "  'question': \"How does Amazon Bedrock's code interpretation feature handle file attachments differently for query answering versus content analysis, and what are the key steps to enable and use this functionality?\",\n",
       "  'ground_truth': 'Amazon Bedrock\\'s code interpretation feature handles file attachments differently for query answering versus content analysis. For query answering and content summarization, users choose \"Attach files to chat (faster)\" option, while for content analysis and metrics provision, they select \"Attach files to code interpreter.\"\\n\\nTo enable and use this functionality:\\n\\n1. Enable Code Interpreter: Create an ActionGroup named \"CodeInterpreterAction\" with \"AMAZON.CodeInterpreter\" as the parent signature and set it to \"ENABLED\" state.\\n\\n2. Prepare the agent: Ensure the agent is updated with these changes.\\n\\n3. Attach files: In the test window, click the paper clip icon. Choose the appropriate function based on your needs (chat or code interpreter). Select the upload method (local computer or Amazon S3).\\n\\n4. For S3 uploads: Specify the S3 path in the API request.\\n\\n5. API Usage: When using the API, send an InvokeAgent request to the Agents for Amazon Bedrock build-time endpoint. Specify the file name, sourceType (s3 or byte_content), and S3Location if applicable.\\n\\nThis process allows users to leverage code interpretation for either interactive querying and summarization or in-depth content analysis and visualization, depending on their specific use case.',\n",
       "  'question_type': 'complex',\n",
       "  'context': 'ired fields for enabling code interpretation with an CreateActionGroup request. CreateAgentActionGroup: { \"actionGroupName\": \"CodeInterpreterAction\", \"parentActionGroupSignature\": \"AMAZON.CodeInterpreter\", \"actionGroupState\": \"ENABLED\" } Test code interpretation in Amazon Bedrock Before you test code interpretation in Amazon Bedrock, make sure to prepare your agent to apply the changes you’ve just made. With code interpretation enabled, when you start to test your agent, you can optionally attach files and choose how you want the files you attach to be used by code interpretation. Depending on your use case, you can ask code interpretation to use the information in the attached files to summarize the contents of the file and to answer queries about the file content during an interactive chat conversation. Or, you can ask code interpretation to analyze the content in the attached files and provide metrics and data visualization reports. Attach files To learn how to attach files for code interpretation, select the tab corresponding to your method of choice and follow the steps: Test code interpretation 759 Amazon Bedrock User Guide Console To attach files for code interpretation, 1. 9 Amazon Bedrock User Guide Console To attach files for code interpretation, 1. If you\\'re not already in the agent builder, do the following: a. Sign in to the AWS Management Console using an IAM role with Amazon Bedrock permissions, and open the Amazon Bedrock console at https:// console.aws.amazon.com/bedrock/. b. Select Agents from the left navigation pane. Then, choose an agent in the Agents section. c. Choose Edit in Agent Builder d. Expand Additional settings and confirm that Code Interpreter is enabled. e. Make sure agent is prepared. 2. If test window is not open, choose Test. 3. In the bottom of the test window, select the paper clip icon to attach files. 4. In the Attach files page, a. For Choose function, specify the following: • If you are attaching files for the agent to use to answer your queries and summarize content, choose Attach files to chat (faster). • If you are attaching files for code interpretation to analyze the content and provide metrics, choose Attach files to code interpreter. alyze the content and provide metrics, choose Attach files to code interpreter. b. For Choose upload method, choose from where you want to upload your files: • If you are uploading from your computer, choose Choose files and select files to attach. • If you are uploading from Amazon S3, choose Browse S3, select files, choose Choose, and then choose Add. 5. Choose Attach. API To test code interpretation, send an InvokeAgent request (see link for request and response formats and field details) with an Agents for Amazon Bedrock build-time endpoint. Test code interpretation 760 Amazon Bedrock User Guide To attach files for agent to use for answering your queries and summarizing the content, specify the following fields: Field Short description name Name of the attached file. sourceType Location of the file to be attached. Specify s3 if your file is located in Amazon S3 bucket. Specify byte_content if your file is located on your computer. S3Location The S3 path where your file is located. Required if the sourceType is S3.'},\n",
       " {'question_id': '6f5c1f87-ff1b-4090-b4cd-163af832a4dc',\n",
       "  'question': 'How can you configure a foundational model for advanced parsing in Amazon Bedrock?',\n",
       "  'ground_truth': 'You can configure a foundational model for advanced parsing in Amazon Bedrock by selecting the custom option for chunking and parsing configurations, enabling the Foundation model, and selecting your preferred foundation model. You can also optionally overwrite the Instructions for the parser to suit your specific needs.',\n",
       "  'question_type': 'simple',\n",
       "  'context': 'mation on the total data that can be parsed using advanced parsing, see Quotas. The following is an example of configuring a foundational model to aid in advanced parsing: Console • Sign in to the AWS Management Console using an IAM role with Amazon Bedrock permissions, and open the Amazon Bedrock console at https://console.aws.amazon.com/bedrock/. • From the left navigation pane, select Knowledge bases. • In the Knowledge bases section, select Create knowledge base. • Provide the knowledge base details such as the name, IAM role for the necessary access permissions, and any tags you want to assign to your knowledge base. • Choose a supported data source and provide the connection configuration details. How content chunking and parsing works 557 Amazon Bedrock User Guide • For chunking and parsing configurations, first choose the custom option and then enable Foundation model and select your preferred foundation model. You can also optionally overwrite the Instructions for the parser to suit your specific needs. tionally overwrite the Instructions for the parser to suit your specific needs. • Continue the steps to complete creating your knowledge base. API { ... \"vectorIngestionConfiguration\": { \"chunkingConfiguration\": { ... }, \"parsingConfiguration\": { // Parse tabular data within docs \"parsingStrategy\": \"BEDROCK_FOUNDATION_MODEL\", \"bedrockFoundationModelConfiguration\": { \"parsingPrompt\": { \"parsingPromptText\": \"string\" }, \"modelArn\": \"string\" } } } } Metadata selection for CSVs When ingesting CSV (comma separate values) files, you have the ability to have the knowledge base treat certain columns as content fields versus metadata fields. Instead of potentially having hundreds or thousands of content/metadata file pairs, you can now have a single CSV file and a corresponding metadata.json file, giving the knowledge base hints as to how to treat each column inside of your CSV. There are limits for document metadata fields/attributes per chunk. See Quotas for knowledge bases Before ingesting a CSV file, make sure: • Your CSV is in RFC4180 format and is UTF-8 encoded. ng a CSV file, make sure: • Your CSV is in RFC4180 format and is UTF-8 encoded. • The first row of your CSV includes header information. • Metadata fields provided in your metadata.json are present as columns in your CSV. • You provide a fileName.csv.metadata.json file with the following format: How content chunking and parsing works 558 Amazon Bedrock User Guide { \"metadataAttributes\": { \"${attribute1}\": \"${value1}\", \"${attribute2}\": \"${value2}\", ... }, \"documentStructureConfiguration\": { \"type\": \"RECORD_BASED_STRUCTURE_METADATA\", \"recordBasedStructureMetadata\": { \"contentFields\": [ { \"fieldName\": \"string\" } ], \"metadataFieldsSpecification\": { \"fieldsToInclude\": [ { \"fieldName\": \"string\" } ], \"fieldsToExclude\": [ { \"fieldName\": \"string\" } ] } } } } The CSV file is parsed one row at a time and the chunking strategy and vector embedding is applied to the content field. Amazon Bedrock knowledge bases currently supports one content field. The content field is split into chunks, and the metadata fields (columns) that are are associated with each chunk are treated as string values.'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import uuid\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "total_chunks = len(chunks)\n",
    "dataset = []\n",
    "\n",
    "generated_question = {\"simple\": [], \"complex\": []}\n",
    "\n",
    "for i in tqdm(range(num_pairs * 2)):\n",
    "    start_id = random.randint(0, total_chunks - 3)\n",
    "    context_chunks = [\n",
    "        chunks[start_id]['content'],\n",
    "        chunks[start_id + 1]['content'],\n",
    "        chunks[start_id + 2]['content']\n",
    "    ]\n",
    "    \n",
    "    context = \" \".join(context_chunks)\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        question_type = \"complex\"\n",
    "    else:\n",
    "        question_type = \"simple\"\n",
    "\n",
    "    user_template = f\"\"\"\n",
    "    Generate a {question_type} question and its answer based on the following context:\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Use the QuestionAnswerGenerator tool to provide the output.\n",
    "    \"\"\"\n",
    "\n",
    "    sys_prompt = [{\"text\": sys_template[question_type]}]\n",
    "    user_prompt = [{\"role\": \"user\", \"content\": [{\"text\": user_template}]}]\n",
    "    temperature = 0.0\n",
    "    top_p = 0.5\n",
    "    inference_config = {\"temperature\": temperature, \"topP\": top_p}\n",
    "\n",
    "    response = bedrock_service.converse_with_tools(\n",
    "        messages=user_prompt,\n",
    "        system_prompt=sys_template[question_type],\n",
    "        tools=tool_config,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=4096\n",
    "    )\n",
    "\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    if stop_reason == 'tool_use':\n",
    "        tool_requests = response['output']['message']['content']\n",
    "\n",
    "        for tool_request in [x for x in tool_requests if 'toolUse' in x]:\n",
    "            if tool_request['toolUse']['name'] == 'QuestionAnswerGenerator':\n",
    "                res = tool_request['toolUse']['input']\n",
    "\n",
    "                qa_item = {\n",
    "                    \"question_id\": f\"{uuid.uuid4()}\",\n",
    "                    \"question\": tool_request['toolUse']['input']['question'],\n",
    "                    \"ground_truth\": tool_request['toolUse']['input']['answer'],\n",
    "                    \"question_type\": question_type,\n",
    "                    \"context\": context\n",
    "                }\n",
    "\n",
    "                with open(output_file, 'a') as f:\n",
    "                    json.dump(qa_item, f)\n",
    "                    f.write('\\n')\n",
    "                \n",
    "                dataset.append(qa_item)\n",
    "\n",
    "dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
